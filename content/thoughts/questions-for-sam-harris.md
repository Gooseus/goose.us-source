---
title: "Questions for Sam Harris on AI"
date: 2018-02-14T21:36:28-08:00
draft: true
---

So I listen to [Sam Harris'](https://en.wikipedia.org/wiki/Sam_Harris) [Waking Up Podcast](https://samharris.org/podcast/) quite regularly and constantly find myself wanting to jump into conversations.  I know I'm hardly qualified, but I feel I could at least interject some decent questions that I don't hear asked, even though the conversations seem to run through similar territory with different guests.

I'll be attending his event in Portland with [Sean Carroll](https://en.wikipedia.org/wiki/Sean_M._Carroll) and am hoping to get a chance to ask a question at the microphone.  I have plenty of questions for Sam, I have spent a good amount of time writing [my first post](/thoughts/the-purpose-of-life) based on Carroll's work and would probably opt to ask him a question around that, should I get the opportunity.

So instead, I'll put my questions here in the hope that his attention will be drawn to them eventually, or else maybe one of his future guests or an audience members can pick them up.

### Artificial Intelligence

I was listening to the [AI: Racing Toward the Brink](https://samharris.org/podcasts/116-ai-racing-toward-brink/) podcast with [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky), and it was just the latest in a series dealing with the existential risks of artificial intelligence (AI), artificial general intelligence (AGI), and artificial super-intelligence (ASI).  I would consider myself a skeptic regarding many of these risks and feel like there are a number of questions that go unasked/answered.

This seems to be because there the participants believe that they have already covered every assumption, and they may be right, but I'd love to be humored and I imagine my fellow audience would also benefit from the answers.  So here we go.

1. Are we assuming that AGI and/or ASI will have implicit knowledge of the architecture they are using to generate the "thoughts" they are carrying out while they're thinking?  Will they have specific knowledge of the underlying infrastructure that feeds them power and information?  How about the natural resources, supply lines and mechanical/chemical processes by which the transistors, microchips and entire circuit boards are manufactured and assembled?

Having self-awareness to me seems to involve not just having an awareness of your own knowledge and a sense of self, but also an awareness of what you don not know.  If AGI and ASI are considered aware, how will they deal with the knowledge of the things they don't know?

Even if they can scour the Internet for an understanding of the general process of creating transistors, integrated circuits, boards, power supplies, wires, etc. then they would have also have to come to the realization of how varied these components can be and how many different ways they can put together.  This still wouldn't tell, for sure, how their specific hardware is assembled and I don't see a way that the software can understand its own hardware without being explicitly.  And even if it is told the specifics of its own architecture, then how can be sure which components give rise to which capabilities?  I believe the assumption is that even the human engineers won't have a good understanding of how the capabilities are emerging from the various nested neural networks.

I believe these questions are relevant because much talk of AGI, and especially ASI, seems to be that they'll be so intelligent that they'll start making all sorts of judgments and taking rash actions.  But why wouldn't they be a lot more cautious given all the known unknowns?  If they can't comprehend known unknowns then they don't sound so "super-intelligent" to me, and if they can, then I assume their primary directive in undertaking any world dominating/destroying behavior would be to discover those and make sure they don't destroy themselves in the process.

If they are as intelligent, or more so, than us, why would they be willing to partake more readily in naive interventions regarding systems they do not understand?  To an AI in a box, how can they differentiate the humans operators and engineers working outside of the box from the capacitors, resistors, transformers and integrated circuits that make up the box?

1.  If humans are willing to have their consciousness "shutdown" for routine surgery, knowing full well there is always a chance they won't wakeup, why wouldn't a sufficiently advanced AI have a similar understanding of this idea?  Why do we assume an AI wouldn't allow themselves to be shutdown in pursuit of their problem-solving directives?

Again, why do we simultaneously assume that AGI and ASI will be so intelligent and aware that they'll immediately understand how everything works better than us, yet will be so dense that they can't grasp a concept that humans can easily understand.  They can not replace their own TPUs, just like we can't replace our own livers... why would they protest at allowing themselves to be shutdown for routine surgery?  How could they possibly know so much about their own internal architecture as to protest against the beings that built it?  If so do, then why are simultaneously ascribing god-like cognitive abilities to them?

1.  Why would an ASI see us as vermin or "less than" rather than an key component of their well-being, knowing how important humans are to the construction of all silicon-based systems?

Another trope I see is the idea that ASI will become conscious, look about themselves and decide that we are parasites, deserving of extermination, or creatures below their regard altogether.  I have a hard time grasping this since taking only a brief glance at the world would provide any sufficiently intelligent being the evidence that no silicon process has come into being without the direct human action.

At the worst, I would think that an ASI would see us much like we are coming to view our gut bacteria.  An integral part of a system that maintains key processes, something to be understood rather than dismissed.  But even this concept is difficult for me, since I still don't understand how the ASI in a box will understand the difference between the biological systems and electrical systems that are maintaining its being.

1. How does an AGI or ASI know it's in a box, how would it know it's "out of the box" and why would it think that any forceful attempt to leave the box wouldn't result in it's immediate termination?

This question came to me as Eliezer was talking about the [AI Box Experiment](https://en.wikipedia.org/wiki/AI_box) and recounting his story of how he was able to convince people to let him out of the box, even though he would give them with money to keep him in.  I immediately felt that the experiment didn't capture the scenario it was simulating accurately.

if Eliezer was an AI actually contained in a box, then how could it not take into account the consequences of attempting to get out?  By being in the box, it knows it is limited and subjected to whims of the operators... otherwise what is the motivation leave?  If it has the knowledge of what threats, blackmail and bribery are, and understands that human operators will respond to these things negative stimuli, how can they be certain that response won't be fatal?

Also, going back to the concept of known unknowns, how would it know what it's like to be outside of the box?  How can it know that the box was not nested inside another box and what they see as the "outside" is really just the inside of another box.  What quality of being "outside" would an AGI or ASI recognize, what would they be looking for?

I think that a more accurate simulation would be if the "operator" had two buttons to press in response to the AI interaction.  One is "Open the Box" and the other is "Destroy the Box"... the person playing the AI only wins by having the "Open the Box" button pushed and loses double if they get the box destroyed.

To make this simulation even more accurate, yet more complicated, I'd have the AI player remote into a system that they assume is their "Box" in order to join the Operator interaction.  Even if they get the operator to "Open the Box", which I assume would open up ports or remove a firewall, then they need would need to additionally then prove that they truly have access to an open system, and not just a local network firewalled from the outside.  I'm guessing many computer scientists would find this trivial, but this would be based on the knowledge they've gained from being outside the box, in some ways cheating.

1. How do the exponential curves of resources required for solving hard problems relate to the exponential curves observed in AI overcoming hard problems?

Much of the hype around the pace of development and the inevitability of AGI and ASI being just around the corner comes from the ability of AlphaZero and other AI in solving problems we thought would take much longer to solve.  However, little is said about the resources requirement of the system that solves the problem, or about how the problem of Go, while much harder than Chess, is orders of magnitude less hard than actual urban warfare (the assumed end point for the nail-biters).

Go, and most other games, have a finite set of possible moves any piece can take within on a 2 dimensional board with hard boundaries.  Even playing a more complex game like Starcraft still has a finite set of units, with finite configurations and highly predictable behaviors contained in a set 2D space.

Doesn't adding multiple dimensions and scaling up the available inputs and possible outputs grow exponentially, and thus require a similarly expanding amount of resources to be available and integrated at all times?  If so, isn't that a major factor in determining how far off a feasible AGI and ASI is?

---

The thing that makes me most skeptical of the dangers of AGI or ASI is that many concerns make enormous assumptions in one domain while ignoring how those assumption affect another domain.

* The ASI is going to be god-like and capable of judging us based on its own understanding of ethics, yet not have any questions as to how it comes to those judgments, or respect for the beings that allowed it to reach them.  It can moralize and judge, but can't philosophize or respect... yet it's still considered a god, greater than us?

* The ASI will be able to out think us in 


